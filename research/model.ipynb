{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape: torch.Size([32, 331, 29])\n",
      "labels.shape: torch.Size([4745])\n",
      "input_lengths: tensor([330, 330, 330, 330, 330, 330, 330, 330, 330, 330, 330, 330, 330, 330,\n",
      "        330, 330, 330, 330, 330, 330, 330, 330, 330, 330, 330, 330, 330, 330,\n",
      "        330, 330, 330, 330], device='cuda:0', dtype=torch.int32)\n",
      "label_lengths: tensor([190, 142,  31, 152, 124, 151, 171, 179, 190, 189,  57, 174, 165, 178,\n",
      "        148, 144, 187, 134, 147,  62, 171, 131, 172, 218, 168, 190, 142, 105,\n",
      "         75, 117, 150, 191], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input_lengths must be of size batch_size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 241\u001b[0m\n\u001b[0;32m    239\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m--> 241\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion, device)\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 194\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_lengths: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_lengths\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_lengths: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel_lengths\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 194\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_lengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    196\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Veera\\anaconda3\\envs\\torch-cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Veera\\anaconda3\\envs\\torch-cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Veera\\anaconda3\\envs\\torch-cuda\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1779\u001b[0m, in \u001b[0;36mCTCLoss.forward\u001b[1;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[0;32m   1778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1779\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1780\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_infinity\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Veera\\anaconda3\\envs\\torch-cuda\\Lib\\site-packages\\torch\\nn\\functional.py:2660\u001b[0m, in \u001b[0;36mctc_loss\u001b[1;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2655\u001b[0m         ctc_loss,\n\u001b[0;32m   2656\u001b[0m         (log_probs, targets, input_lengths, target_lengths),\n\u001b[0;32m   2657\u001b[0m         log_probs, targets, input_lengths, target_lengths,\n\u001b[0;32m   2658\u001b[0m         blank\u001b[38;5;241m=\u001b[39mblank, reduction\u001b[38;5;241m=\u001b[39mreduction, zero_infinity\u001b[38;5;241m=\u001b[39mzero_infinity\n\u001b[0;32m   2659\u001b[0m     )\n\u001b[1;32m-> 2660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_infinity\u001b[49m\n\u001b[0;32m   2662\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input_lengths must be of size batch_size"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio.transforms import MelSpectrogram, Resample\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the Feature Extractor\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=128, out_channels=32, kernel_size=11, stride=2, padding=5)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.hswish = nn.Hardswish()\n",
    "        self.se_module = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Conv1d(32, 8, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 32, kernel_size=1),\n",
    "            nn.Hardsigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.hswish(x)\n",
    "        x = self.se_module(x) * x\n",
    "        return x\n",
    "\n",
    "# Define the Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Hardswish()\n",
    "        )\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm1d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm1d(64),\n",
    "                nn.ReLU()\n",
    "            ) for _ in range(3)\n",
    "        ])\n",
    "        self.out_layer = nn.Conv1d(64, 128, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.downsample(x)\n",
    "        for conv in self.conv_layers:\n",
    "            x = conv(x) + x  # Residual connection\n",
    "        x = self.out_layer(x)\n",
    "        return x\n",
    "\n",
    "# Define the CTC Projector\n",
    "class CTCProjector(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CTCProjector, self).__init__()\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(2, 0, 1)  # (batch, channels, time) -> (time, batch, channels)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Define the ASR Model\n",
    "class ASRModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ASRModel, self).__init__()\n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "        self.encoder = Encoder()\n",
    "        self.ctc_projector = CTCProjector(num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.ctc_projector(x)\n",
    "        return x\n",
    "\n",
    "# Dataset and DataLoader\n",
    "class LibriSpeechDataset(Dataset):\n",
    "    def __init__(self, root_dir, sample_rate=16000, n_mels=128):\n",
    "        self.root_dir = root_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.file_paths = []\n",
    "        self.labels = {}\n",
    "        self.resample = Resample(orig_freq=sample_rate, new_freq=sample_rate)\n",
    "        self.mel_spectrogram = MelSpectrogram(sample_rate=sample_rate, n_mels=n_mels)\n",
    "        self.load_dataset()\n",
    "\n",
    "    def load_dataset(self):\n",
    "        for transcript_path in glob.glob(os.path.join(self.root_dir, '**', '*.trans.txt'), recursive=True):\n",
    "            with open(transcript_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split(' ', 1)\n",
    "                    file_id = parts[0]\n",
    "                    transcript = parts[1]\n",
    "                    audio_path = os.path.join(os.path.dirname(transcript_path), file_id + '.flac')\n",
    "                    if os.path.exists(audio_path):\n",
    "                        self.file_paths.append(audio_path)\n",
    "                        self.labels[audio_path] = transcript\n",
    "                    else:\n",
    "                        print(f\"Audio file {audio_path} not found, skipping.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(file_path)\n",
    "            if sample_rate != self.sample_rate:\n",
    "                waveform = self.resample(waveform)\n",
    "            mel_spec = self.mel_spectrogram(waveform).squeeze(0)\n",
    "            label = self.labels[file_path]\n",
    "            label = self.text_to_labels(label)\n",
    "            return mel_spec, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {file_path}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def text_to_labels(self, text):\n",
    "        return torch.tensor([ord(char) - ord('a') + 1 for char in text.lower() if char.isalpha()])\n",
    "\n",
    "# Define the collate function\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item[0] is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    mel_specs, labels = zip(*batch)\n",
    "    \n",
    "    # Determine the maximum length of mel spectrograms\n",
    "    max_length = max(mel_spec.size(1) for mel_spec in mel_specs)\n",
    "    \n",
    "    # Truncate or pad mel spectrograms to the maximum length\n",
    "    padded_mel_specs = []\n",
    "    for mel_spec in mel_specs:\n",
    "        if (mel_spec.size(1) > max_length):\n",
    "            mel_spec = mel_spec[:, :max_length]\n",
    "        else:\n",
    "            pad_size = max_length - mel_spec.size(1)\n",
    "            mel_spec = F.pad(mel_spec, (0, pad_size), \"constant\", 0)\n",
    "        padded_mel_specs.append(mel_spec)\n",
    "    \n",
    "    mel_specs = torch.stack(padded_mel_specs, dim=0)\n",
    "    \n",
    "    label_lengths = torch.tensor([len(label) for label in labels], dtype=torch.int32)\n",
    "    labels = torch.cat(labels)\n",
    "    \n",
    "    # Compute the actual lengths of the mel spectrograms after padding\n",
    "    mel_spec_lengths = torch.tensor([mel_spec.size(1) for mel_spec in padded_mel_specs], dtype=torch.int32)\n",
    "    \n",
    "    return mel_specs, labels, mel_spec_lengths, label_lengths\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = LibriSpeechDataset(root_dir=\"../audio_datasets/train/LibriSpeech/\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "val_dataset = LibriSpeechDataset(root_dir=\"../audio_datasets/val/LibriSpeech/\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Training and evaluation functions with debugging\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for mel_specs, labels, mel_spec_lengths, label_lengths in train_loader:\n",
    "        mel_specs, labels = mel_specs.to(device), labels.to(device)\n",
    "        mel_spec_lengths, label_lengths = mel_spec_lengths.to(device), label_lengths.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(mel_specs)\n",
    "        outputs = outputs.log_softmax(2)\n",
    "        outputs = outputs.permute(1, 0, 2)  # (batch, time, class) -> (time, batch, class)\n",
    "        \n",
    "        # Calculate the length of the sequence over time for each sample in the batch\n",
    "        input_lengths = mel_spec_lengths // 4  # Assuming downsampling factor of 4\n",
    "        \n",
    "        # Debugging prints\n",
    "        print(f'outputs.shape: {outputs.shape}')\n",
    "        print(f'labels.shape: {labels.shape}')\n",
    "        print(f'input_lengths: {input_lengths}')\n",
    "        print(f'label_lengths: {label_lengths}')\n",
    "        \n",
    "        loss = criterion(outputs, labels, input_lengths, label_lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(train_loader)\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            if batch[0] is None:\n",
    "                continue  # Skip batches where all items are invalid\n",
    "            \n",
    "            mel_specs, labels, mel_spec_lengths, label_lengths = batch\n",
    "            mel_specs, labels = mel_specs.to(device), labels.to(device)\n",
    "            mel_spec_lengths, label_lengths = mel_spec_lengths.to(device), label_lengths.to(device)\n",
    "            \n",
    "            outputs = model(mel_specs)\n",
    "            outputs = outputs.log_softmax(2)\n",
    "            outputs = outputs.permute(1, 0, 2)  # (batch, time, class) -> (time, batch, class)\n",
    "            \n",
    "            # Calculate the length of the sequence over time for each sample in the batch\n",
    "            input_lengths = mel_spec_lengths\n",
    "            \n",
    "            # Debugging prints\n",
    "            print(f'outputs.shape: {outputs.shape}')\n",
    "            print(f'labels.shape: {labels.shape}')\n",
    "            print(f'input_lengths: {input_lengths}')\n",
    "            print(f'label_lengths: {label_lengths}')\n",
    "\n",
    "            loss = criterion(outputs, labels, input_lengths, label_lengths)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(val_loader)\n",
    "\n",
    "# Model Configuration\n",
    "num_classes = len(\" abcdefghijklmnopqrstuvwxyz'\") + 1  # Adjust based on the dataset used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ASRModel(num_classes).to(device)\n",
    "criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
